{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSI6TUG5wnjZ",
        "outputId": "e3a43484-d5b9-4f56-bc40-bdf5906acc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: u\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "Hit Enter to continue: c\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_ne_chunker_tab ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger_tab Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "Hit Enter to continue: h\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "Hit Enter to continue: q\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: \n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "Hit Enter to continue: \n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_ne_chunker_tab ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger_tab Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "Hit Enter to continue: \n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "Hit Enter to continue: \n",
            "  [ ] punkt_tab........... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "Hit Enter to continue: \n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets (JSON)\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "Hit Enter to continue: \n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download_shell()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('gutenberg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JMAhW-4xfbB",
        "outputId": "51a5fc92-8828-49bb-f8ca-4bdb0d211bd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gb = nltk.corpus.gutenberg\n",
        "print(\"Gutenberg files : \", gb.fileids())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUx7cawKxk2H",
        "outputId": "ebb981e9-4d65-4f0f-a1b1-7c235f3a527b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gutenberg files :  ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')"
      ],
      "metadata": {
        "id": "AZq96D-Xxtyo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(macbeth)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMTdpgiKxwSx",
        "outputId": "5b5391dc-5349-4836-e7d3-386465c1f2fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23140"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth [:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dXq2qS7xzSG",
        "outputId": "56842831-aed7-4a0b-8d5b-872710c82de7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[',\n",
              " 'The',\n",
              " 'Tragedie',\n",
              " 'of',\n",
              " 'Macbeth',\n",
              " 'by',\n",
              " 'William',\n",
              " 'Shakespeare',\n",
              " '1603',\n",
              " ']']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "macbeth_sents = nltk.corpus.gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sents[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP5Q4ttLx3aZ",
        "outputId": "3aa74678-4e02-4d2f-e001-687bef21f698"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[',\n",
              "  'The',\n",
              "  'Tragedie',\n",
              "  'of',\n",
              "  'Macbeth',\n",
              "  'by',\n",
              "  'William',\n",
              "  'Shakespeare',\n",
              "  '1603',\n",
              "  ']'],\n",
              " ['Actus', 'Primus', '.'],\n",
              " ['Scoena', 'Prima', '.'],\n",
              " ['Thunder', 'and', 'Lightning', '.'],\n",
              " ['Enter', 'three', 'Witches', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = nltk.Text(macbeth)\n",
        "text.concordance('Stage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMdU2K5qyFMP",
        "outputId": "74ea603d-cf44-4a6d-d828-b7e2df8adf72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying 3 of 3 matches:\n",
            "nts with Dishes and Seruice ouer the Stage . Then enter Macbeth Macb . If it we\n",
            "with mans Act , Threatens his bloody Stage : byth ' Clock ' tis Day , And yet d\n",
            " struts and frets his houre vpon the Stage , And then is heard no more . It is \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.common_contexts(['Stage'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRtG4DYayIU_",
        "outputId": "35d65acf-063d-4f18-c624-6da8c97c6e1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the_. bloody_: the_,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.similar('Stage')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D4zky3zyK1n",
        "outputId": "f95bb990-fa68-49ec-b2f6-3dc215207e13"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "day time face warre ayre king bleeding man reuolt serieant like\n",
            "knowledge broyle shew head spring heeles hare thane skie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhLUDqSJyNGQ",
        "outputId": "eb0ae41a-b45b-45a9-b29f-36ac32aaa3b2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " ('the', 531),\n",
              " (':', 477),\n",
              " ('and', 376),\n",
              " ('I', 333),\n",
              " ('of', 315),\n",
              " ('to', 311),\n",
              " ('?', 241)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TVQilZdyPjH",
        "outputId": "04455e71-1037-417a-ecb0-19374d8ebdf4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = set(nltk.corpus.stopwords.words('english'))\n",
        "print(len(sw))\n",
        "list(sw)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtcEh3PKyR-I",
        "outputId": "5360f5ec-2dcf-47f4-eada-744483b129d8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "198\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['now', 'themselves', 've', 'when', 'had', 'no', 'these', 'on', 'once', 'isn']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_filtered = [w for w in macbeth if w.lower() not in sw]\n",
        "len(macbeth_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wEkluFTyUMR",
        "outputId": "38f44759-7640-4519-ce46-f13c27eeb94f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14946"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd = nltk.FreqDist(macbeth_filtered)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtN45rOqyW5g",
        "outputId": "2df66deb-35cf-4ade-8699-fbaaf8d4cb7a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1962),\n",
              " ('.', 1235),\n",
              " (\"'\", 637),\n",
              " (':', 477),\n",
              " ('?', 241),\n",
              " ('Macb', 137),\n",
              " ('haue', 117),\n",
              " ('-', 100),\n",
              " ('Enter', 80),\n",
              " ('thou', 63)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "macbeth_filtered2 = [w.lower() for w in macbeth if w.lower() not in sw and w.lower() not in punctuation]\n",
        "fd = nltk.FreqDist(macbeth_filtered2)\n",
        "fd.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZ8VMwpGyZvY",
        "outputId": "264899fa-a6ff-4ad0-dbb0-008fad507480"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('macb', 137),\n",
              " ('haue', 122),\n",
              " ('thou', 90),\n",
              " ('enter', 81),\n",
              " ('shall', 68),\n",
              " ('macbeth', 62),\n",
              " ('vpon', 62),\n",
              " ('thee', 61),\n",
              " ('macd', 58),\n",
              " ('vs', 57)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "long_words = [w for w in macbeth if len(w)> 12]\n",
        "sorted(long_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW6cw5suyeNP",
        "outputId": "a392daee-a744-4aba-f352-7a8f728641a9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Assassination',\n",
              " 'Chamberlaines',\n",
              " 'Distinguishes',\n",
              " 'Gallowgrosses',\n",
              " 'Metaphysicall',\n",
              " 'Northumberland',\n",
              " 'Voluptuousnesse',\n",
              " 'commendations',\n",
              " 'multitudinous',\n",
              " 'supernaturall',\n",
              " 'vnaccompanied']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ious_words = [w for w in macbeth if 'ious' in w]\n",
        "ious_words = set(ious_words)\n",
        "sorted(ious_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YPY-Ykqylsx",
        "outputId": "52e9a630-3f57-4219-b5ab-e9bf8e302532"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Auaricious',\n",
              " 'Gracious',\n",
              " 'Industrious',\n",
              " 'Iudicious',\n",
              " 'Luxurious',\n",
              " 'Malicious',\n",
              " 'Obliuious',\n",
              " 'Pious',\n",
              " 'Rebellious',\n",
              " 'compunctious',\n",
              " 'furious',\n",
              " 'gracious',\n",
              " 'pernicious',\n",
              " 'pernitious',\n",
              " 'pious',\n",
              " 'precious',\n",
              " 'rebellious',\n",
              " 'sacrilegious',\n",
              " 'serious',\n",
              " 'spacious',\n",
              " 'tedious']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bgrms = nltk.FreqDist(nltk.bigrams(macbeth_filtered2))\n",
        "bgrms.most_common(15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W66tCn6kyoUh",
        "outputId": "090c84c2-6a53-4c7b-d7d1-791303284fee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('enter', 'macbeth'), 16),\n",
              " (('exeunt', 'scena'), 15),\n",
              " (('thane', 'cawdor'), 13),\n",
              " (('knock', 'knock'), 10),\n",
              " (('st', 'thou'), 9),\n",
              " (('thou', 'art'), 9),\n",
              " (('lord', 'macb'), 9),\n",
              " (('haue', 'done'), 8),\n",
              " (('macb', 'haue'), 8),\n",
              " (('good', 'lord'), 8),\n",
              " (('let', 'vs'), 7),\n",
              " (('enter', 'lady'), 7),\n",
              " (('wee', 'l'), 7),\n",
              " (('would', 'st'), 6),\n",
              " (('macbeth', 'macb'), 6)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tgrms = nltk.FreqDist(nltk.trigrams (macbeth_filtered2))\n",
        "tgrms.most_common(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r3VxRbdyrt_",
        "outputId": "09b8e558-be0d-4cf0-99dc-120be3c348da"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('knock', 'knock', 'knock'), 6),\n",
              " (('enter', 'macbeth', 'macb'), 5),\n",
              " (('enter', 'three', 'witches'), 4),\n",
              " (('exeunt', 'scena', 'secunda'), 4),\n",
              " (('good', 'lord', 'macb'), 4),\n",
              " (('three', 'witches', '1'), 3),\n",
              " (('exeunt', 'scena', 'tertia'), 3),\n",
              " (('thunder', 'enter', 'three'), 3),\n",
              " (('exeunt', 'scena', 'quarta'), 3),\n",
              " (('scena', 'prima', 'enter'), 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf8')\n",
        "raw[:75]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LN3ZYGYJyuax",
        "outputId": "e2a5bbbe-3c8a-44a6-a53b-7a866aa5ac07"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "response = request.urlopen(url)\n",
        "raw = response.read().decode('utf-8')\n",
        "raw[:75]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HZvmQ5nvyyGD",
        "outputId": "83e3513f-5c8c-420d-fcb6-a3bf1aaace10"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'*** START OF THE PROJECT GUTENBERG EBOOK 2554 ***\\n\\n\\n\\n\\nCRIME AND PUNISHMENT\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize (raw)\n",
        "webtext = nltk.Text (tokens)\n",
        "webtext[:12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS0U1FplzDPB",
        "outputId": "65757667-af5b-4cf3-b894-1984e0e7b9c5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['*',\n",
              " '*',\n",
              " '*',\n",
              " 'START',\n",
              " 'OF',\n",
              " 'THE',\n",
              " 'PROJECT',\n",
              " 'GUTENBERG',\n",
              " 'EBOOK',\n",
              " '2554',\n",
              " '*',\n",
              " '*']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
        "html = request.urlopen(url).read().decode('utf8')\n",
        "html[:120]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xxfYvcDdzGi5",
        "outputId": "f463e010-08e5-437a-ec96-18e87b1717bc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<hea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "raw = BeautifulSoup(html, \"lxml\").get_text()\n",
        "tokens = nltk.word_tokenize(raw)\n",
        "text = nltk.Text(tokens)"
      ],
      "metadata": {
        "id": "XDdyKat4zJw-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgxmaYqXzMtB",
        "outputId": "c9358064-7d2a-4219-9fac-c0847a240e33"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "reviews = nltk.corpus.movie_reviews\n",
        "documents = [(list(reviews.words(fileid)), category)\n",
        "for category in reviews.categories()\n",
        "for fileid in reviews.fileids(category)]\n",
        "random.shuffle(documents)"
      ],
      "metadata": {
        "id": "TGwg6VpvzPHR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_review = ' '.join(documents[0][0])\n",
        "print(first_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IjCGFcdzV5_",
        "outputId": "1813a56e-4841-4a42-d505-7fc2072c428f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summary five liberal iowa graduate students share a house and carry on a tradition of a large sunday dinner . for a year now they have been inviting a guest for dinner and discussion . on this occasion one of the roommates , pete ( ron eldard ) , brings home a stranger who picked him up when his car broke down . the stranger is zack ( bill paxton ) who appears like a decent enough good samaritan . nothing could be further from the truth . after some dinner conversation it appears that zack is , among other things , a racist \" you know my grandfather once said that if he knew you coloreds were going to be so much trouble we ' d have picked the damn cotton ourselves \" , and an anti - semite . an argument escalates at the dinner table and leads to violence . after zack assaults marc ( jonathan penner ) and pete , zack is killed . the roommates are at odds with what to do . marc , pete and paulie ( annabeth gish ) immediately wish to call the police . luke ( courtney b . vance ) and jude ( cameron diaz ) suggest something different . after more discussion they agree to dispose of the body and cover it up . they do not want to be sent to jail for what they feel was a justifiable homicide . when it is done the roommates realize that it was easier than they thought . so easy in fact that they decide that every sunday their dinner guest would be someone who opposes of their views . if they all agree that the guest deserves to die , they will poison them and bury them in the back yard . the back yard fills up rather quickly , its ' victims including ( among others ) a right to life extremist , an anti - environmentalist and a homophobe . by the time the roommates get to their tenth victim they begin to question their methods . are they giving the guests a chance ? are they even giving them good food anymore ? this film is what i like to call a \" hidden treasure \" . this film is such a pleasant surprise . a well written , devilish satire that is funny , sexy and original . this film should be a hit , but i guess the fact that it apparently had no marketing wouldn ' t give it much of a chance . i had never heard of it until i saw a copy at my video store . stacy title does a wonderful job directing . she somehow finds the time to give all of the characters enough screen time to develop a distinct personality . > from the five members of the house to other key characters including sheriff stanley ( nora dunn ) , the local law closing in on the truth , and the dream dinner guest , norman arbuthnot ( ron perlman ) , a rush limbaugh clone who should be the roommates easiest kill . perlman gives the films ' best performance . he finds the right note as the right wing big mouth who simultaneously offends and gains followers , and may just inadvertently talk his way out of death . the other actors are convincing as well , as all of the roommates deal with their murderous ways . some of the characters appear to be heading down the road towards insanity , while others grieve their decisions . when friends ask me to recommend a movie , this is usually the first one i think of . i am not saying that this is my favorite movie ( although it is in my top ten ) , but because the film was such a wonderful surprise , i have a fondness for it . i also try to recommend films like this : ones that have not been seen by many people , but should .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CoiTC7hBzYNm",
        "outputId": "fb9a7bc3-ecfc-43ab-b3a1-039046db0a2e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pos'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = nltk.FreqDist(w.lower() for w in reviews.words())\n",
        "word_features = list(all_words)"
      ],
      "metadata": {
        "id": "tsAomZ44zbG5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def document_features(document, word_features):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features['{}'.format(word)] = (word in document_words)\n",
        "    return features"
      ],
      "metadata": {
        "id": "nuK37KhSzmLj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "featuresets = [(document_features(d,word_features), c) for (d,c) in documents]\n",
        "len(featuresets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6grA-ErzpGo",
        "outputId": "f1df8f27-c219-4673-b6a2-0780a6ccd49c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)"
      ],
      "metadata": {
        "id": "_4Y4wed8z02Y"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, est_set = featuresets[1500:], featuresets[:500]\n",
        "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "print(nltk.classify.accuracy(classifier, test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0UqetWp0I0u",
        "outputId": "b1857752-a0c0-45b8-b9fc-7f6b7db04f36"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.show_most_informative_features(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6utgCfiW0nFf",
        "outputId": "440a2a54-804a-4ce2-aeb7-082e42fb877d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "             wonderfully = True              pos : neg    =     11.8 : 1.0\n",
            "                   waste = True              neg : pos    =     11.4 : 1.0\n",
            "                villains = True              neg : pos    =      9.6 : 1.0\n",
            "               fantastic = True              pos : neg    =      9.5 : 1.0\n",
            "             outstanding = True              pos : neg    =      9.5 : 1.0\n",
            "                    draw = True              pos : neg    =      9.0 : 1.0\n",
            "                  social = True              pos : neg    =      9.0 : 1.0\n",
            "               laughable = True              neg : pos    =      8.5 : 1.0\n",
            "                  eating = True              neg : pos    =      8.1 : 1.0\n",
            "                   inept = True              neg : pos    =      8.1 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Bài tập áp dụng**"
      ],
      "metadata": {
        "id": "CIItOIAN3Z6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Viết chương trình Python với thư viện NLTK để liệt kê các tên của copus.\n",
        "import nltk\n",
        "\n",
        "print(nltk.corpus.__dir__())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BqM9e520poW",
        "outputId": "0b4a7db2-e9c9-46df-8ebd-288df8c69724"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__path__', '__file__', '__cached__', '__builtins__', '__annotations__', 're', 'reader', 'CorpusReader', 'CategorizedCorpusReader', 'PlaintextCorpusReader', 'find_corpus_fileids', 'TaggedCorpusReader', 'CMUDictCorpusReader', 'ConllChunkCorpusReader', 'WordListCorpusReader', 'PPAttachmentCorpusReader', 'SensevalCorpusReader', 'IEERCorpusReader', 'ChunkedCorpusReader', 'SinicaTreebankCorpusReader', 'BracketParseCorpusReader', 'IndianCorpusReader', 'ToolboxCorpusReader', 'TimitCorpusReader', 'YCOECorpusReader', 'MacMorphoCorpusReader', 'SyntaxCorpusReader', 'AlpinoCorpusReader', 'RTECorpusReader', 'StringCategoryCorpusReader', 'EuroparlCorpusReader', 'CategorizedBracketParseCorpusReader', 'CategorizedTaggedCorpusReader', 'CategorizedPlaintextCorpusReader', 'PortugueseCategorizedPlaintextCorpusReader', 'tagged_treebank_para_block_reader', 'PropbankCorpusReader', 'VerbnetCorpusReader', 'BNCCorpusReader', 'ConllCorpusReader', 'XMLCorpusReader', 'NPSChatCorpusReader', 'SwadeshCorpusReader', 'WordNetCorpusReader', 'WordNetICCorpusReader', 'SwitchboardCorpusReader', 'DependencyCorpusReader', 'NombankCorpusReader', 'IPIPANCorpusReader', 'Pl196xCorpusReader', 'TEICorpusView', 'KNBCorpusReader', 'ChasenCorpusReader', 'CHILDESCorpusReader', 'AlignedCorpusReader', 'TimitTaggedCorpusReader', 'LinThesaurusCorpusReader', 'SemcorCorpusReader', 'FramenetCorpusReader', 'UdhrCorpusReader', 'SentiWordNetCorpusReader', 'SentiSynset', 'TwitterCorpusReader', 'NKJPCorpusReader', 'CrubadanCorpusReader', 'MTECorpusReader', 'ReviewsCorpusReader', 'OpinionLexiconCorpusReader', 'ProsConsCorpusReader', 'CategorizedSentencesCorpusReader', 'ComparativeSentencesCorpusReader', 'PanLexLiteCorpusReader', 'NonbreakingPrefixesCorpusReader', 'UnicharsCorpusReader', 'MWAPPDBCorpusReader', 'PanlexSwadeshCorpusReader', 'BCP47CorpusReader', 'util', 'LazyCorpusLoader', 'RegexpTokenizer', 'abc', 'alpino', 'bcp47', 'brown', 'cess_cat', 'cess_esp', 'cmudict', 'comtrans', 'comparative_sentences', 'conll2000', 'conll2002', 'conll2007', 'crubadan', 'dependency_treebank', 'extended_omw', 'floresta', 'framenet15', 'framenet', 'gazetteers', 'genesis', 'gutenberg', 'ieer', 'inaugural', 'indian', 'jeita', 'knbc', 'lin_thesaurus', 'mac_morpho', 'machado', 'masc_tagged', 'movie_reviews', 'multext_east', 'names', 'nps_chat', 'opinion_lexicon', 'ppattach', 'product_reviews_1', 'product_reviews_2', 'pros_cons', 'ptb', 'qc', 'reuters', 'rte', 'senseval', 'sentence_polarity', 'sentiwordnet', 'shakespeare', 'sinica_treebank', 'state_union', 'stopwords', 'subjectivity', 'swadesh', 'swadesh110', 'swadesh207', 'switchboard', 'timit', 'timit_tagged', 'toolbox', 'treebank', 'treebank_chunk', 'treebank_raw', 'twitter_samples', 'udhr', 'udhr2', 'universal_treebanks', 'verbnet', 'webtext', 'wordnet', 'wordnet31', 'wordnet2021', 'wordnet2022', 'wordnet_ic', 'words', 'propbank', 'nombank', 'propbank_ptb', 'nombank_ptb', 'semcor', 'nonbreaking_prefixes', 'perluniprops', 'demo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Viết chương trình Python với thư viện NLTK để liệt kê danh sách các stopword bằng các ngôn ngữ khác nhau\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(stopwords.fileids())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKP7kt761SXn",
        "outputId": "7e5de81f-eeca-454a-9e3e-6a4341f1853c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['albanian', 'arabic', 'azerbaijani', 'basque', 'belarusian', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Viết chương trình Python với thư viện NLTK để kiểm tra danh sách các stopword bằng các ngôn ngữ khác nhau.\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "languages = ['english', 'french', 'german', 'spanish']\n",
        "\n",
        "for lang in languages:\n",
        "    print(f\"Stopwords in {lang}:\")\n",
        "    print(stopwords.words(lang)[:10])\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVt0WodG1cZa",
        "outputId": "3e43fce2-3e60-4611-96fc-37aeb2a97300"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords in english:\n",
            "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an']\n",
            "\n",
            "Stopwords in french:\n",
            "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']\n",
            "\n",
            "Stopwords in german:\n",
            "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an']\n",
            "\n",
            "Stopwords in spanish:\n",
            "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Viết chương trình Python với thư viện NLTK để loại bỏ các stopword từ một văn bản đã cho.\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"loai bo stopword.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "print(\"Original:\", words)\n",
        "print(\"Filtered:\", filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXBCvbpV1p1o",
        "outputId": "c67c45a4-67b7-4c32-ba58-3c4747fd6a65"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: ['loai', 'bo', 'stopword', '.']\n",
            "Filtered: ['loai', 'bo', 'stopword', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Viết chương trình Python với thư viện NLTK bỏ qua các stopword từ danh sách các stopword.\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "original_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "excluded_words = {'not', 'no', 'against'}\n",
        "filtered_stopwords = original_stopwords - excluded_words\n",
        "\n",
        "print(\"Stopwords after removal:\", list(filtered_stopwords)[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K90j0Mwp15Gn",
        "outputId": "1a399503-3c48-43cc-afc4-050d2693336f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopwords after removal: ['now', 'themselves', 've', 'when', 'had', 'these', 'on', 'once', 'isn', 'needn']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Viết một chương trình Python với thư viện NLTK để tìm định nghĩa và ví dụ của một từ đã cho bằng WordNet từ Wikipedia,\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "word = \"computer\"\n",
        "\n",
        "# Lấy định nghĩa và ví dụ\n",
        "synsets = wordnet.synsets(word)\n",
        "for synset in synsets:\n",
        "    print(f\"Definition: {synset.definition()}\")\n",
        "    print(f\"Examples: {synset.examples()}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hEqiKAF2CIJ",
        "outputId": "4453c98b-b8ef-45af-e708-f687c39b9013"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition: a machine for performing calculations automatically\n",
            "Examples: []\n",
            "\n",
            "Definition: an expert at calculation (or at operating calculating machines)\n",
            "Examples: []\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Viết chương trình Python với thư viện NLTK để tìm tập hợp các từ đồng nghĩa và trái nghĩa của một từ nào đó.\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "word = \"happy\"\n",
        "\n",
        "synonyms = set()\n",
        "antonyms = set()\n",
        "\n",
        "for synset in wordnet.synsets(word):\n",
        "    for lemma in synset.lemmas():\n",
        "        synonyms.add(lemma.name())\n",
        "        if lemma.antonyms():\n",
        "            antonyms.add(lemma.antonyms()[0].name())\n",
        "\n",
        "print(\"Synonyms:\", synonyms)\n",
        "print(\"Antonyms:\", antonyms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4MgBOLo2L3c",
        "outputId": "63bccc69-d340-417e-cad6-229efb5efad2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms: {'happy', 'glad', 'well-chosen', 'felicitous'}\n",
            "Antonyms: {'unhappy'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Viết chương trình Python với thư viện NLTK để có cái nhìn tổng quan về bộ tag, chi tiết của một tag cụ thể trong bộ tag và chi tiết về một số bộ tag liên quan, sử dụng biểu thức chính quy.\n",
        "import nltk\n",
        "\n",
        "nltk.download('tagsets_json')\n",
        "\n",
        "nltk.help.upenn_tagset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tZM25Wk2UVh",
        "outputId": "31c38e92-9438-4de5-c7c9-01fb63c1bfd8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets_json to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$: dollar\n",
            "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
            "'': closing quotation mark\n",
            "    ' ''\n",
            "(: opening parenthesis\n",
            "    ( [ {\n",
            "): closing parenthesis\n",
            "    ) ] }\n",
            ",: comma\n",
            "    ,\n",
            "--: dash\n",
            "    --\n",
            ".: sentence terminator\n",
            "    . ! ?\n",
            ":: colon or ellipsis\n",
            "    : ; ...\n",
            "CC: conjunction, coordinating\n",
            "    & 'n and both but either et for less minus neither nor or plus so\n",
            "    therefore times v. versus vs. whether yet\n",
            "CD: numeral, cardinal\n",
            "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
            "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
            "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
            "DT: determiner\n",
            "    all an another any both del each either every half la many much nary\n",
            "    neither no some such that the them these this those\n",
            "EX: existential there\n",
            "    there\n",
            "FW: foreign word\n",
            "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
            "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
            "    terram fiche oui corporis ...\n",
            "IN: preposition or conjunction, subordinating\n",
            "    astride among uppon whether out inside pro despite on by throughout\n",
            "    below within for towards near behind atop around if like until below\n",
            "    next into if beside ...\n",
            "JJ: adjective or numeral, ordinal\n",
            "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
            "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
            "    multilingual multi-disciplinary ...\n",
            "JJR: adjective, comparative\n",
            "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
            "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
            "    cozier creamier crunchier cuter ...\n",
            "JJS: adjective, superlative\n",
            "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
            "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
            "    dearest deepest densest dinkiest ...\n",
            "LS: list item marker\n",
            "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
            "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
            "    two\n",
            "MD: modal auxiliary\n",
            "    can cannot could couldn't dare may might must need ought shall should\n",
            "    shouldn't will would\n",
            "NN: noun, common, singular or mass\n",
            "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
            "    investment slide humour falloff slick wind hyena override subhumanity\n",
            "    machinist ...\n",
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n",
            "NNPS: noun, proper, plural\n",
            "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
            "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
            "    Apache Apaches Apocrypha ...\n",
            "NNS: noun, common, plural\n",
            "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
            "    divestitures storehouses designs clubs fragrances averages\n",
            "    subjectivists apprehensions muses factory-jobs ...\n",
            "PDT: pre-determiner\n",
            "    all both half many quite such sure this\n",
            "POS: genitive marker\n",
            "    ' 's\n",
            "PRP: pronoun, personal\n",
            "    hers herself him himself hisself it itself me myself one oneself ours\n",
            "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
            "PRP$: pronoun, possessive\n",
            "    her his mine my our ours their thy your\n",
            "RB: adverb\n",
            "    occasionally unabatingly maddeningly adventurously professedly\n",
            "    stirringly prominently technologically magisterially predominately\n",
            "    swiftly fiscally pitilessly ...\n",
            "RBR: adverb, comparative\n",
            "    further gloomier grander graver greater grimmer harder harsher\n",
            "    healthier heavier higher however larger later leaner lengthier less-\n",
            "    perfectly lesser lonelier longer louder lower more ...\n",
            "RBS: adverb, superlative\n",
            "    best biggest bluntest earliest farthest first furthest hardest\n",
            "    heartiest highest largest least less most nearest second tightest worst\n",
            "RP: particle\n",
            "    aboard about across along apart around aside at away back before behind\n",
            "    by crop down ever fast for forth from go high i.e. in into just later\n",
            "    low more off on open out over per pie raising start teeth that through\n",
            "    under unto up up-pp upon whole with you\n",
            "SYM: symbol\n",
            "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
            "TO: \"to\" as preposition or infinitive marker\n",
            "    to\n",
            "UH: interjection\n",
            "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
            "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
            "    man baby diddle hush sonuvabitch ...\n",
            "VB: verb, base form\n",
            "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
            "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
            "    boost brace break bring broil brush build ...\n",
            "VBD: verb, past tense\n",
            "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
            "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
            "    speculated wore appreciated contemplated ...\n",
            "VBG: verb, present participle or gerund\n",
            "    telegraphing stirring focusing angering judging stalling lactating\n",
            "    hankerin' alleging veering capping approaching traveling besieging\n",
            "    encrypting interrupting erasing wincing ...\n",
            "VBN: verb, past participle\n",
            "    multihulled dilapidated aerosolized chaired languished panelized used\n",
            "    experimented flourished imitated reunifed factored condensed sheared\n",
            "    unsettled primed dubbed desired ...\n",
            "VBP: verb, present tense, not 3rd person singular\n",
            "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
            "    appear tend stray glisten obtain comprise detest tease attract\n",
            "    emphasize mold postpone sever return wag ...\n",
            "VBZ: verb, present tense, 3rd person singular\n",
            "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
            "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
            "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
            "WDT: WH-determiner\n",
            "    that what whatever which whichever\n",
            "WP: WH-pronoun\n",
            "    that what whatever whatsoever which who whom whosoever\n",
            "WP$: WH-pronoun, possessive\n",
            "    whose\n",
            "WRB: Wh-adverb\n",
            "    how however whence whenever where whereby whereever wherein whereof why\n",
            "``: opening quotation mark\n",
            "    ` ``\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai danh từ đã cho.\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "noun1 = wordnet.synset(\"dog.n.01\")\n",
        "noun2 = wordnet.synset(\"cat.n.01\")\n",
        "\n",
        "similarity = noun1.wup_similarity(noun2)\n",
        "print(f\"Similarity between {noun1} and {noun2}: {similarity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxSFA3kE2n_h",
        "outputId": "f005fd16-71ec-415e-868a-96fdea36c169"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between Synset('dog.n.01') and Synset('cat.n.01'): 0.8571428571428571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Viết chương trình Python với thư viện NLTK để so sánh sự giống nhau của hai động từ đã cho.\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "verb1 = wordnet.synset(\"run.v.01\")\n",
        "verb2 = wordnet.synset(\"walk.v.01\")\n",
        "\n",
        "similarity = verb1.wup_similarity(verb2)\n",
        "print(f\"Similarity between {verb1} and {verb2}: {similarity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWdS5FoN2wTZ",
        "outputId": "ffb32343-12be-41c5-a13c-5a9ddb154ad8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between Synset('run.v.01') and Synset('walk.v.01'): 0.2857142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Viết chương trình Python với thư viện NLTK để tìm số lượng tên nam và nữ trong các tên kho ngữ liệu. In tên 10 nam và nữ đầu tiên. Lưu ý: Kho văn bản tên chứa tổng cộng khoảng 2943 nam (male.txt) và 5001 nữ (Female.txt) tên. Kho được biên soạn bởi Kantrowitz, Ross.\n",
        "import nltk\n",
        "from nltk.corpus import names\n",
        "\n",
        "nltk.download('names')\n",
        "\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "\n",
        "print(f\"Number of male names: {len(male_names)}\")\n",
        "print(f\"Number of female names: {len(female_names)}\")\n",
        "\n",
        "print(\"First 10 male names:\", male_names[:10])\n",
        "print(\"First 10 female names:\", female_names[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ab5Okk-25Ey",
        "outputId": "9138a98a-572f-41b5-d1e9-a278f18b1eb9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of male names: 2943\n",
            "Number of female names: 5001\n",
            "First 10 male names: ['Aamir', 'Aaron', 'Abbey', 'Abbie', 'Abbot', 'Abbott', 'Abby', 'Abdel', 'Abdul', 'Abdulkarim']\n",
            "First 10 female names: ['Abagael', 'Abagail', 'Abbe', 'Abbey', 'Abbi', 'Abbie', 'Abby', 'Abigael', 'Abigail', 'Abigale']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Viết chương trình Python với thư viện NLTK để in 15 kết hợp ngẫu nhiên đầu tiên được gắn nhãn nam và được gắn nhãn tên nữ từ kho tên.\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import names\n",
        "\n",
        "nltk.download('names')\n",
        "\n",
        "male_names = [(name, 'male') for name in names.words('male.txt')]\n",
        "female_names = [(name, 'female') for name in names.words('female.txt')]\n",
        "\n",
        "# Gộp và xáo trộn danh sách\n",
        "combined_names = male_names + female_names\n",
        "random.shuffle(combined_names)\n",
        "\n",
        "print(\"15 random names:\")\n",
        "print(combined_names[:15])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT9DS5Jj3FpS",
        "outputId": "5c643c91-2526-4fa9-c521-02de52a99527"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15 random names:\n",
            "[('Ventura', 'female'), ('Nani', 'female'), ('Mala', 'female'), ('Zarah', 'female'), ('Kalil', 'male'), ('Casey', 'female'), ('Konstance', 'female'), ('Alaine', 'female'), ('Lambert', 'male'), ('Durant', 'male'), ('Keene', 'male'), ('Luke', 'male'), ('Dania', 'female'), ('Shelia', 'female'), ('Obie', 'male')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Viết chương trình Python với thư viện NLTK để trích xuất ký tự cuối cùng của tất cả các tên được gắn nhãn và tạo mảng mới với chữ cái cuối cùng của mỗi tên và nhãn được liên kết.\n",
        "import nltk\n",
        "from nltk.corpus import names\n",
        "\n",
        "nltk.download('names')\n",
        "\n",
        "def extract_features(name):\n",
        "    return name[-1]  # Lấy ký tự cuối cùng của tên\n",
        "\n",
        "male_names = names.words('male.txt')\n",
        "female_names = names.words('female.txt')\n",
        "\n",
        "features = [(extract_features(name), 'male') for name in male_names] + \\\n",
        "           [(extract_features(name), 'female') for name in female_names]\n",
        "\n",
        "print(\"First 10 extracted features:\", features[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUQYwu-M3MsM",
        "outputId": "8b094ae6-0793-4b7d-cde9-3b214eed409b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 extracted features: [('r', 'male'), ('n', 'male'), ('y', 'male'), ('e', 'male'), ('t', 'male'), ('t', 'male'), ('y', 'male'), ('l', 'male'), ('l', 'male'), ('m', 'male')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}